apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-storage
  namespace: soperator
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: bert-finetuning-script-fixed
  namespace: soperator
data:
  finetune_bert.py: |
    import os
    import sys
    import subprocess
    
    print("Installing compatible versions...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", 
                          "transformers==4.36.0",
                          "datasets==2.14.0", 
                          "accelerate==0.25.0",
                          "--no-cache-dir"])
    
    import torch
    from transformers import (
        BertForMaskedLM, 
        BertTokenizer, 
        DataCollatorForLanguageModeling,
        TrainingArguments,
        Trainer
    )
    from datasets import Dataset
    import numpy as np
    from datetime import datetime
    
    print(f"\nStarting BERT Fine-tuning - {datetime.now()}")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Count: {torch.cuda.device_count()}")
    
    # Create domain-specific training data (financial domain)
    training_texts = [
        "The stock market showed strong bullish performance today.",
        "Federal Reserve announced hawkish monetary policy stance.",
        "Bitcoin cryptocurrency surged past resistance levels.",
        "Earnings reports exceeded analyst expectations significantly.",
        "Oil futures dropped amid OPEC production increases.",
        "Inflation data triggered bond yield movements.",
        "Corporate bonds showed increased institutional demand.",
        "Merger activity accelerated in pharmaceutical sector.",
        "Gold commodities indicated safe haven demand.",
        "Central banks coordinated dovish policy measures.",
        "Market volatility increased amid economic uncertainty.",
        "Treasury yields climbed on inflation concerns.",
        "Equity markets rallied on positive earnings.",
        "Currency pairs showed increased trading volume.",
        "Derivatives market expanded with new products.",
    ] * 50  # Repeat for more training data
    
    print(f"\nTraining on {len(training_texts)} financial domain examples")
    
    # Initialize tokenizer and model
    model_name = 'bert-base-uncased'
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertForMaskedLM.from_pretrained(model_name)
    
    # Move model to GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    # Tokenize data
    def tokenize_function(examples):
        return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)
    
    # Create dataset
    dataset = Dataset.from_dict({'text': training_texts})
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    # Data collator for MLM
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=True,
        mlm_probability=0.15
    )
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir="/model/checkpoint",
        overwrite_output_dir=True,
        num_train_epochs=2,  # Reduced for faster demo
        per_device_train_batch_size=8,  # Smaller batch size
        save_steps=50,
        save_total_limit=2,
        prediction_loss_only=True,
        logging_steps=10,
        warmup_steps=50,
        logging_dir='/model/logs',
        dataloader_num_workers=2,
        run_name="bert-financial-finetuning",
        save_strategy="epoch",
        evaluation_strategy="no",
        fp16=True,
        report_to=[]  # Disable wandb/tensorboard
    )
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    # Train
    print("\nStarting fine-tuning...")
    print("This will take approximately 5-10 minutes...")
    train_result = trainer.train()
    
    # Save the fine-tuned model
    print("\nSaving fine-tuned model...")
    model.save_pretrained("/model/fine-tuned-bert")
    tokenizer.save_pretrained("/model/fine-tuned-bert")
    
    # Test the fine-tuned model
    print("\n" + "="*60)
    print("TESTING FINE-TUNED MODEL")
    print("="*60)
    
    test_sentences = [
        "The stock market is [MASK] today.",
        "Interest rates affect [MASK] yields.",
        "Bitcoin showed [MASK] momentum.",
        "The Federal Reserve announced [MASK] policy.",
    ]
    
    model.eval()
    for sentence in test_sentences:
        inputs = tokenizer(sentence, return_tensors='pt').to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = outputs.logits
        
        mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
        mask_token_logits = predictions[0, mask_token_index, :]
        top_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()
        
        print(f"\n'{sentence}'")
        print("Top predictions:", [tokenizer.decode([t]) for t in top_tokens[:3]])
    
    print("\n" + "="*60)
    print("FINE-TUNING COMPLETED SUCCESSFULLY!")
    print(f"Model saved to: /model/fine-tuned-bert")
    print(f"Training loss: {train_result.training_loss:.4f}")
    print("Ready for inference and comparison tasks!")
    print("="*60)
---
apiVersion: batch/v1
kind: Job
metadata:
  name: bert-finetuning-fixed
  namespace: soperator
spec:
  template:
    metadata:
      labels:
        app: bert-finetuning-fixed
    spec:
      restartPolicy: Never
      nodeSelector:
        node.kubernetes.io/instance-type: gpu-h100-sxm
      containers:
      - name: finetuning
        image: nvcr.io/nvidia/pytorch:23.10-py3
        command: ["python", "/scripts/finetune_bert.py"]
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: HF_HOME
          value: "/tmp/hf_cache"
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
          limits:
            memory: "32Gi"
            cpu: "8" 
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: script
          mountPath: /scripts
        - name: model-storage
          mountPath: /model
      volumes:
      - name: script
        configMap:
          name: bert-finetuning-script-fixed
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-storage
