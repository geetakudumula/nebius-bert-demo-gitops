apiVersion: v1
kind: ConfigMap
metadata:
  name: bert-inference-standalone-script
  namespace: soperator
data:
  inference_server.py: |
    import os
    import sys
    import torch
    from transformers import BertTokenizer, BertForMaskedLM
    from flask import Flask, request, jsonify
    import logging
    import time
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    app = Flask(__name__)
    
    # Global variables
    model = None
    tokenizer = None
    device = None
    
    def initialize_model():
        global model, tokenizer, device
        
        try:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"Using device: {device}")
            
            if torch.cuda.is_available():
                logger.info(f"GPU Name: {torch.cuda.get_device_name(0)}")
            
            # For demo purposes, we'll use base model but note it should be fine-tuned
            logger.info("Loading BERT model...")
            logger.info("NOTE: Due to PVC mount issues, using base model.")
            logger.info("In production, this would load the fine-tuned model from Task 1.")
            
            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
            model = BertForMaskedLM.from_pretrained('bert-base-uncased')
            
            # Simulate some fine-tuning effect for demo
            logger.info("Model loaded successfully!")
            logger.info("Task 2 Inference Service Ready")
            
            model.to(device)
            model.eval()
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize model: {e}")
            return False
    
    @app.route('/health', methods=['GET'])
    def health():
        return jsonify({
            'status': 'healthy',
            'model_loaded': model is not None,
            'model_note': 'Using base model due to PVC issue - see logs',
            'gpu_available': torch.cuda.is_available(),
            'device': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
            'task': 'Task 2 - Inference Deployment'
        })
    
    @app.route('/predict', methods=['POST'])
    def predict():
        if model is None:
            return jsonify({'error': 'Model not loaded'}), 503
            
        try:
            data = request.json
            text = data.get('text', '')
            
            inputs = tokenizer(text, return_tensors='pt').to(device)
            
            start_time = time.time()
            with torch.no_grad():
                outputs = model(**inputs)
                predictions = outputs.logits
            inference_time = (time.time() - start_time) * 1000
            
            mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
            
            if len(mask_token_index) == 0:
                return jsonify({'error': 'No [MASK] token found'}), 400
            
            mask_token_logits = predictions[0, mask_token_index, :]
            probs = torch.softmax(mask_token_logits, dim=1)
            top_5 = torch.topk(probs, 5, dim=1)
            
            results = []
            for i in range(5):
                token_id = top_5.indices[0, i].item()
                score = top_5.values[0, i].item()
                token = tokenizer.decode([token_id])
                results.append({
                    'token': token,
                    'score': float(score)
                })
            
            return jsonify({
                'input': text,
                'predictions': results,
                'inference_time_ms': round(inference_time, 2),
                'model_note': 'Base model (PVC issue documented)'
            })
            
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    if __name__ == '__main__':
        logger.info("="*60)
        logger.info("TASK 2: INFERENCE DEPLOYMENT")
        logger.info("="*60)
        if initialize_model():
            app.run(host='0.0.0.0', port=8080, debug=False)
        else:
            sys.exit(1)
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bert-inference-standalone
  namespace: soperator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bert-inference-standalone
  template:
    metadata:
      labels:
        app: bert-inference-standalone
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: gpu-h100-sxm
      containers:
      - name: bert-inference
        image: nvcr.io/nvidia/pytorch:23.10-py3
        command: ["/bin/bash", "-c"]
        args:
          - |
            pip install flask transformers==4.36.0 --no-cache-dir
            python /app/inference_server.py
        ports:
        - containerPort: 8080
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
          limits:
            memory: "32Gi"
            cpu: "8"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: inference-script
          mountPath: /app
      volumes:
      - name: inference-script
        configMap:
          name: bert-inference-standalone-script
---
apiVersion: v1
kind: Service
metadata:
  name: bert-inference-standalone
  namespace: soperator
spec:
  selector:
    app: bert-inference-standalone
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
