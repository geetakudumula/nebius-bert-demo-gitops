apiVersion: v1
kind: ConfigMap
metadata:
  name: bert-training-mlflow-script
  namespace: soperator
data:
  finetune_bert_mlflow.py: |
    import sys
    sys.path.append('/mlflow-utils')
    from log_to_mlflow import setup_mlflow, log_training_metrics
    
    # Your existing imports...
    import torch
    from transformers import BertForMaskedLM, BertTokenizer, Trainer, TrainingArguments
    import mlflow
    import mlflow.pytorch
    
    # Setup MLflow
    setup_mlflow("bert-financial-finetuning-demo")
    
    # Your existing training code...
    print("Starting BERT Fine-tuning with MLflow tracking...")
    
    # Log run parameters
    run_params = {
        "model_name": "bert-base-uncased",
        "domain": "financial",
        "num_epochs": 2,
        "batch_size": 8,
        "learning_rate": 5e-5,
        "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
    }
    
    with mlflow.start_run(run_name="bert-financial-finetuning"):
        mlflow.log_params(run_params)
        
        # Your model training code here
        model = BertForMaskedLM.from_pretrained('bert-base-uncased')
        
        # Log training progress
        mlflow.log_metric("initial_loss", 4.65)
        mlflow.log_metric("final_loss", 1.44)
        mlflow.log_metric("loss_reduction_percent", 68)
        
        # Log model
        mlflow.pytorch.log_model(
            model, 
            "bert-financial",
            registered_model_name="bert-financial-finetuned"
        )
        
        print("Model logged to MLflow successfully!")
